---
title: "STAT 600 - HW 2"
author: "Kevin Korsurat"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(knitr)
library(Rcpp)
library(RcppArmadillo)
library(ggplot2)
library(latex2exp)
library(foreach)
library(doParallel)

path <- "/Users/kevin-imac/Desktop/Github - Repo/HW2Optim/src/"
# path <- "/Users/kevinkvp/Desktop/Github Repo/HW2Optim/src/"
sourceCpp(paste0(path, "main.cpp"))
```

All Rcpp/RcppArmadillo can be found in my [\textcolor{red}{GitHub}](https://github.com/skorsu/HW2Optim).

```{r}

### User-defined functions -----------------------------------------------------
meanSD <- function(x, dplace = 5){
  mm <- round(mean(x), digits = dplace)
  ss <- round(sd(x), digits = dplace)
  paste0(mm, " (SD = ", ss, ")")
}
```


## Question 1

### (a)

First, consider the likelihood and the log-likelihood function.

$$\begin{aligned}
\text{L}\left(\theta\right) &= \prod_{i=1}^{n}\frac{1}{\pi\left(1 + \left(x_{i} - \theta\right)^{2}\right)} \\
l\left(\theta\right) &= \log\left(L\left(\theta\right)\right) \\
&= \log\left(\prod_{i=1}^{n}\frac{1}{\pi\left(1 + \left(x_{i} - \theta\right)^{2}\right)}\right) \\
&= \sum_{i=1}^{n}\log\left(\frac{1}{\pi\left(1 + \left(x_{i} - \theta\right)^{2}\right)}\right) \\
&= -\sum_{i=1}^{n}\log\left(\pi\left(1 + \left(x_{i} - \theta\right)^{2}\right)\right) \\
&= - n\log\left(\pi \right) - \sum_{i=1}^{n}\log\left(1 + \left(x_{i} - \theta\right)^{2}\right)
\end{aligned}$$

Then, consider the derivative of the log-likelihood, $l'\left(\theta\right)$.

$$\begin{aligned}
l'\left(\theta\right) &= \frac{d}{d\theta}l\left(\theta\right) \\
&= -\sum_{i=1}^{n}\frac{1}{1 + \left(x_{i} - \theta\right)^{2}} \left[\frac{d}{d\theta}\left(x_{i} - \theta\right)^{2}\right] \\
&= 2\sum_{i=1}^{n}\frac{x_{i} - \theta}{1 + \left(x_{i} - \theta\right)^{2}}
\end{aligned}$$

The Figure 1 depicts the plot of the derivative of the log-likelihood. According to the plot, we notice that in the range of [-10, 10], the solution to $\frac{d}{d\theta}l\left(\theta\right) = 0$ might be somewhere around 1.

```{r, fig.cap = "The plot of the derivative of the log-likelihood on the original dataset."}

### Q1 -------------------------------------------------------------------------
#### Plot the derivative of the log-likelihood
dat <- c(-8.86, -6.82, -4.03, -2.84, 0.14, 0.19, 0.24, 0.27, 0.49, 0.62, 0.76, 1.09,
         1.18, 1.32, 1.36, 1.58, 1.58, 1.78, 2.13, 2.15, 2.36, 4.05, 4.11, 4.12, 
         6.83)
rangeTheta <- seq(-10, 10, 0.01)
data.frame(theta = rangeTheta, dll = sapply(rangeTheta, dloglik, x = dat)) %>%
  ggplot(aes(x = theta, y = dll)) +
  geom_line() +
  theme_bw() +
  labs(x = TeX("\\theta"), y = TeX("l'(\\theta)"), 
       title = "The plot of the derivative of the log-likelihood based on 25 daat points")
```

\newpage

### (b)

This is the second derivation for the log-likelihood function.

$$\begin{aligned}
l''\left(\theta\right) &= \frac{d}{d\theta}l'\left(\theta\right) \\
&= \frac{d}{d\theta} 2\sum_{i=1}^{n}\frac{x_{i} - \theta}{1 + \left(x_{i} - \theta\right)^{2}} \\
&= 2 \sum_{i=1}^{n} \frac{d}{d\theta} \frac{x_{i} - \theta}{1 + \left(x_{i} - \theta\right)^{2}} \\
&= 2 \sum_{i=1}^{n} \frac{-1 + \left(x_{i} - \theta\right)^{2}}{\left(1 + \left(x_{i} - \theta\right)^{2}\right)^{2}}
\end{aligned}$$

For the Fisher's score, it is obviously shows in this [\textcolor{red}{link}](https://www.ma.imperial.ac.uk/~das01/MyWeb/M3S3/Handouts/WorkedExample-Cauchy.pdf) that the Fisher's score is $\frac{n}{2}$.

### (c)

Below are the result from each methods. Note that I have set the $\epsilon$ to be $1 \times 10^{-5}$.

```{r}

### Run all methods
eps_set <- 1e-5
bs_dat <- bisect_q1(min(dat), max(dat), dat, eps = eps_set)
nr_dat <- nr_q1(x0 = 0, dat = dat, eps = eps_set)
fs_dat <- fs_q1(x0 = 0, dat = dat, eps = eps_set)
sc_dat <- sc_q1(x0 = 0, x1 = 1e-5, dat = dat, eps = eps_set)

### Create the table
data.frame(theta = c(bs_dat$xt, nr_dat$xt, fs_dat$xt, sc_dat$xt), 
           iter = c(bs_dat$n_iter, nr_dat$n_iter, fs_dat$n_iter, sc_dat$n_iter)) %>%
  `rownames<-`(c("Bisection", "Newton-Raphson", "Fisher Scoring", "Secant Method")) %>%
  kable(digits = 5, col.names = c("$\\hat\\theta$", "Number of iteration"),
        caption = "The result from each methods with only 25 observations.") 
```


### (d)

For the convergence criteria used in this problem, I decided to employ the absolute convergence criterion, as the $x^{(t)}$ might be close to 0 in some iterations, as indicated by the plot shown in part (a). Additionally, the value of x is neither too tiny nor too huge compared to $\epsilon$.

### (e)

According to the result shown in part (c), I can conclude that $\hat{\theta}$ is 1.1879. For the standard error, we can calculate by using the Fisher's Information. Hence, the standard error is $\sqrt{\frac{2}{25}} = 0.28284$.

### (f)

According to the plot shown in part (a), I have initialized the starting point as 0 for the Newton-Raphson and Fisher methods, as we expect the final answer to be around 1. For the same reason, I have set $x_{0}$ and $x_{1}$ to be 0 and 1, respectively, for the secant method.

For the bisection method, I have initialized the interval as $\left(\min(\boldsymbol{x}), \max\left(\boldsymbol{x}\right)\right)$, as we believe that the answer must be somewhere in the range of the data.

#### Sensitivity Analysis

For each method, I will run the algorithm 1,000 times with a random starting point. First, I will consider the bisection method. The way I set the initial interval is by randomly selecting two numbers from Uniform[-10, 10]. I have classified the initial interval into two groups: one covering the median of the data (1.18) and the other not covering the median, as the Maximum Likelihood Estimate (MLE) of $\theta$ is a median. Table 3 shows that regardless of the size of the interval, as long as the initial interval covers the solution $\left(l'(\theta) = 0 \right)$, the algorithm can find the optimal points. The size of the interval is the one that controls the number of iterations. Therefore, I would say that this method is quite sensitive to the initial interval.

```{r}

### Run Sensitivity: Bisection
set.seed(213, kind = "L'Ecuyer-CMRG")
registerDoParallel(5)
senBisection <- foreach(t = 1:1000, .combine = rbind) %dopar% {
  
  x <- runif(2, -10, 10)
  algResult <- bisect_q1(min(x), max(x), dat, eps = 1e-5)
  c(min(x), max(x), algResult$xt, algResult$n_iter)
  
}
stopImplicitCluster()

data.frame(senBisection) %>%
  mutate(coverAns = ifelse(X1 <= 1.18 & 1.18 <= X2, "Cover", "Not Cover")) %>%
  group_by(coverAns) %>%
  summarise(optAns = meanSD(X3), Iter = meanSD(X4)) %>%
  kable(digits = 5, col.names = c("Interval", "Average $\\hat\\theta$", "Average Number of iteration"),
        caption = "The result of the bisection method classified by the starting interval") 
```

Next, I will consider the Newton-Raphson method. According to Table 3, I believe the starting point plays an important role. I have sampled the starting point from Uniform[-10, 10]. The results show that if you choose an appropriate starting point, the algorithm will converge to the correct answer. Otherwise, it will not converge. The reason is that for each iteration that we update, the denominator for the step involves the second derivative of the log-likelihood. If the second derivative equals 0, the result will diverge.

```{r}

### Run Sensitivity: Newton-Raphson
set.seed(213, kind = "L'Ecuyer-CMRG")
registerDoParallel(5)
senNR <- foreach(t = 1:1000, .combine = rbind) %dopar% {
  
  x0init <- runif(1, -10, 10)
  algResult <- tryCatch(
        { nr_q1(x0 = x0init, dat = dat, eps = 1e-5) },
        error = function(cond) {
          list(xt = NA, n_iter = NA)
        }
)
  c(x0init, algResult$xt, algResult$n_iter)
  
}
stopImplicitCluster()

data.frame(senNR) %>%
  mutate(converge = ifelse(is.na(X2), "No", "Yes")) %>%
  group_by(converge) %>%
  summarise(n = n(), x2 = meanSD(X2),  x3 = meanSD(X3)) %>%
  kable(digits = 5, col.names = c("Convergence", "Frequency", "Average $\\hat\\theta$", "Average Number of iteration"),
        caption = "The result of the Newton-Raphson method classified by the convergence") 
```

Figure 2 reveals that, provided we choose the appropriate starting point, we can still reach the correct answer. The number of iterations does not heavily depend on the starting points. Therefore, the conclusion I would draw for this method is that it is quite sensitive to the choice of starting point. However, if we can figure out the possible range of the appropriate starting point, this algorithm is not sensitive.

```{r, fig.cap = "Optimal point for each starting points via Newton-Raphson method."}

data.frame(senNR) %>%
  filter(! is.na(X2)) %>%
  ggplot(aes(x = X1, y = X2, color = factor(X3))) +
  geom_point() +
  ylim(1.187, 1.189) +
  theme_bw() +
  theme(legend.position = "bottom") +
  scale_x_continuous(breaks = seq(-10, 10, by = 1)) +
  labs(color = "Number of iterations", x = TeX("Starting point ($x_{0}$)"),
       y = TeX("$\\hat{\\theta}$"), 
       title = "The optimal point for each possible starting pont via Newton-Raphson Method")
```

For the Fisher scoring method, we found that the algorithm always converges to the correct answer, as shown in Table 4. This is in contrast to the Newton-Raphson method, where certain initial values can cause the algorithm to diverge. The reason for this distinction lies in the fact that the denominator for each updating step in the Fisher algorithm is the number of observations, which cannot be zero. Note that I have ranlomly choose the starting pomt from Unif[-10, 10].

```{r}

### Run Sensitivity: Fisher Scoring
set.seed(213, kind = "L'Ecuyer-CMRG")
registerDoParallel(5)
senFisher <- foreach(t = 1:1000, .combine = rbind) %dopar% {
  
  x0init <- runif(1, -10, 10)
  algResult <- fs_q1(x0 = x0init, dat = dat, eps = 1e-5)
  c(x0init, algResult$xt, algResult$n_iter)
  
}
stopImplicitCluster()

data.frame(t(apply(senFisher[, -1], 2, meanSD))) %>%
  kable(digits = 5, col.names = c("Average $\\hat\\theta$", "Average Number of iteration"),
        caption = "The result of the Fisher Scoring method")
```

Furthermore, Figure 3 illustrates that the farther the starting point is from the optimal value, the more iterations the algorithm might require. Hence, I will conclude that the Fisher scoring method is not sensitive to the starting point.

```{r, fig.cap = "The relationship between the starting point and the number of iteration for the Fisher Scoring method."}

data.frame(senFisher[, -2]) %>%
  ggplot(aes(x = X1, y = X2)) +
  geom_point() +
  geom_vline(xintercept = 1.18) +
  theme_bw() +
  labs(title = TeX("The starting point ($x_{0}$) and the number of iterations"),
       x = TeX("Starting point ($x_{0}$)"), y = "Iteration")
```

Finally, for the secant method, I sampled two starting points, $x_{0}$ and $x_{1}$, from Unif[-10,10]. The results, as revealed in Table 4, are similar to the Newton-Raphson method; the algorithm converges to the optimal points when provided with good starting points. Figure 4 displays the range of possible starting points that enable the algorithm to converge. Therefore, I would like to conclude that the algorithm is not sensitive when we have good knowledge about the range of plausible starting points. Otherwise, the result is sensitive to the starting point, as the algorithm might not converge.

```{r}

### Run Sensitivity: Secant
set.seed(213, kind = "L'Ecuyer-CMRG")
registerDoParallel(5)
senSecant <- foreach(t = 1:1000, .combine = rbind) %dopar% {
  
  x0init <- runif(1, -10, 10)
  x1init <- runif(1, -10, 10)
  algResult <- tryCatch(
        { sc_q1(x0 = x0init, x1 = x1init, dat = dat, eps = 1e-5) },
        error = function(cond) {
          list(xt = NA, n_iter = NA)
        })
  c(x0init, x1init, algResult$xt, algResult$n_iter)
  
}
stopImplicitCluster()

data.frame(senSecant) %>%
  mutate(converge = ifelse(is.na(X3), "No", "Yes")) %>%
  group_by(converge) %>%
  summarise(n = n(), x2 = meanSD(X3),  x3 = meanSD(X4)) %>%
  kable(digits = 5, col.names = c("Convergence", "Frequency", "Average $\\hat\\theta$", "Average Number of iteration"),
        caption = "The result of the Secant method classified by the convergence") 
```

```{r, fig.cap = "The plot shows the range of the starting point for the secant method in order to make the algorithm converge."}

data.frame(senSecant) %>%
  mutate(converge = ifelse(is.na(X3), "No", "Yes")) %>%
  ggplot(aes(x = X1, y = X2, color = converge)) +
  geom_point() +
  scale_color_manual(values = c("grey90", "red")) +
  theme_bw() +
  theme(legend.position = "bottom") +
  labs(color = "Converged", x = TeX("$x_{0}"), y = TeX("$x_{1}"),
       title = "The range of the starting point for the secant method.")
```


### (g)

Below is the result when using all data points.

```{r}

### Additional data
add_dat <- c(-8.34, -1.73, -0.40, -0.24, 0.60, 0.94, 1.05, 1.06, 1.45, 1.50, 
             1.54, 1.72, 1.74, 1.88, 2.04, 2.16, 2.39, 3.01, 3.01, 3.08, 4.66,
             4.99, 6.01, 7.06, 25.45)

### Run all methods with complete data
eps_set <- 1e-5
bs_cdat <- bisect_q1(min(c(dat, add_dat)), max(c(dat, add_dat)), dat = c(dat, add_dat), eps = eps_set)
nr_cdat <- nr_q1(x0 = 0.5, dat = c(dat, add_dat), eps = eps_set)
fs_cdat <- fs_q1(x0 = 0.5, dat = c(dat, add_dat), eps = eps_set)
sc_cdat <- sc_q1(x0 = 0, x1 = 0.5, dat = c(dat, add_dat), eps = eps_set)

### Create the table
data.frame(theta = c(bs_cdat$xt, nr_cdat$xt, fs_cdat$xt, sc_cdat$xt), 
           iter = c(bs_cdat$n_iter, nr_cdat$n_iter, fs_cdat$n_iter, sc_cdat$n_iter)) %>%
  `rownames<-`(c("Bisection", "Newton-Raphson", "Fisher Scoring", "Secant Method")) %>%
  kable(digits = 5, col.names = c("$\\hat\\theta$", "Number of iteration"),
        caption = "The result from each methods with all 50 observations.") 
```

Similar to the part (e), the standard error, which can be calculated from the Fisher Infomation, is $\sqrt{\frac{2}{50}} = 0.2$.

## Question 2

This proof will follow closely to the proof appears in the book. (page 31-32). First, by applying the equation (2.27), we have $\epsilon^{(t+2)} \approx d^{(t+1)}\epsilon^{(t+1)}\epsilon^{(t)}$ and $\lim_{t \rightarrow \infty} d^{(t+1)} = d$ where d is defined in the book. Also, we have $\lim_{t \rightarrow \infty} \frac{|\epsilon^{(t+1)}|}{|\epsilon^{(t)}|^{\beta_{1}}} = c_{1}$ where $\beta_{1} = \frac{1 + \sqrt{5}}{2}$. The rate of convergence for the two-step secant can be found by solving $\beta_{2}$ in $\lim_{t \rightarrow \infty} \frac{|\epsilon^{(t+2)}|}{|\epsilon^{(t)}|^{\beta_{2}}} = c_{2}$ where $c_{2}$ is some constant.

$$\begin{aligned}
\lim_{t \rightarrow \infty} \frac{|\epsilon^{(t+2)}|}{|\epsilon^{(t)}|^{\beta_{2}}} &= \lim_{t \rightarrow \infty} \frac{|d^{(t+1)}\epsilon^{(t+1)}\epsilon^{(t)}|}{|\epsilon^{(t)}|^{\beta_{2}}} \\
&= \frac{\lim_{t \rightarrow \infty}|d^{(t+1)}| \lim_{t \rightarrow \infty} |\epsilon^{(t+1)}| \lim_{t \rightarrow \infty}|\epsilon^{(t)}|}{\lim_{t \rightarrow \infty}|\epsilon^{(t)}|^{\beta_{2}}} \\ 
&= \frac{d c_{1}\lim_{t \rightarrow \infty} |\epsilon^{(t)}|^{\beta_{1}} \lim_{t \rightarrow \infty}|\epsilon^{(t)}|}{\lim_{t \rightarrow \infty}|\epsilon^{(t)}|^{\beta_{2}}} \\ 
&= d c_{1}\lim_{t \rightarrow \infty} |\epsilon^{(t)}|^{1 + \beta_{1} - \beta_{2}}
\end{aligned}$$

Therefore, we have $d c_{1}\lim_{t \rightarrow \infty} |\epsilon^{(t)}|^{1 + \beta_{1} - \beta_{2}} = c_{2}$, which is equivalent to $\lim_{t \rightarrow \infty} |\epsilon^{(t)}|^{1 + \beta_{1} - \beta_{2}} = \frac{c_{2}}{d c_{1}}$ By apply the same logic as in the equation (2.28), $\frac{c_{2}}{d c_{1}}$ is a positive constant; therefore, $1 + \beta_{1} - \beta_{2} = 0 \leftrightarrow \beta_{2} = 1 + \beta_{1} = 1 + \frac{1 + \sqrt{5}}{2} = \frac{3 + \sqrt{5}}{2} \approx 2.61803$.

Since the rate of convergence for the one-step Newton-Raphson method is 2, which is less than 2.61803, the two-step secant method is better than the one-step Newton-Raphson method.

## Question 3

### (a)

I will denote $\beta_{0} + \beta_{1}x_{i1} + \beta_{2}x_{i2}$ as $\boldsymbol{x}_{i}\boldsymbol{\beta}$. Since we know that $Y_{i} \sim \text{Ber}\left(\frac{\exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)}{1 + \exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)}\right)$, then the likelihood and the log-likelihood can be derived as below.

$$\begin{aligned}
\text{L}\left(\boldsymbol{\beta}\right) &= \prod_{i=1}^{n}\left[\frac{\exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)}{1 + \exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)}\right]^{y_{i}}\left[1-\frac{\exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)}{1 + \exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)}\right]^{1 - y_{i}} \\
&= \prod_{i=1}^{n}\frac{\exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)^{y_{i}}}{1 + \exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)} \\
l\left(\boldsymbol{\beta}\right) &= \log\left(\text{L}\left(\boldsymbol{\beta}\right)\right) \\
&= \sum_{i=1}^{n}\left[y_{i}\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right) - \log\left(1 + \exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)\right)\right]
\end{aligned}$$

### (b)

First, consider the first derivative of the log-likelihood w.r.t. $\boldsymbol{\beta}$, or the gradient.

$$\begin{aligned}
\triangledown_{\boldsymbol{\beta}}l\left(\boldsymbol{\beta}\right) &= \frac{d}{d\boldsymbol{\beta}} l\left(\boldsymbol{\beta}\right) \\ 
&= \sum_{i=1}^{n}\left[y_{i}\frac{d}{d\boldsymbol{\beta}}\boldsymbol{x}_{i}\boldsymbol{\beta} - \frac{d}{d\boldsymbol{\beta}} \log\left(1 + \exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)\right)\right] \\
&= \sum_{i=1}^{n}\left[y_{i}\boldsymbol{x}_{i}^{T} - \frac{\exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)}{1 + \exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)}\boldsymbol{x}_{i}^{T}\right] \\
&= \sum_{i=1}^{n}\left[y_{i} - \frac{\exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)}{1 + \exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)}\right]\boldsymbol{x}_{i}^{T}
\end{aligned}$$

We can rewrite the formula above in a matrix form as $\triangledown_{\boldsymbol{\beta}}l\left(\boldsymbol{\beta}\right) = \boldsymbol{X}^{T}\left(\boldsymbol{Y} - \hat{\boldsymbol{Y}}\right)$, where $\hat{\boldsymbol{Y}}$ is a vector consisted of $\frac{\exp\left(\boldsymbol{x}_{i}^{T}\boldsymbol{\beta}\right)}{1 + \exp\left(\boldsymbol{x}_{i}^{T}\boldsymbol{\beta}\right)}$ since we can think this quantity as a predicted probability of success for the observation i.

Now, we will consider the Hessian for the log-likelihood.

$$\begin{aligned}
H\left(\boldsymbol{\beta}\right) &= \triangledown_{\boldsymbol{\beta}}\left(\triangledown_{\boldsymbol{\beta}}l\left(\boldsymbol{\beta}\right)\right) \\
&= \triangledown_{\boldsymbol{\beta}} \sum_{i=1}^{n}\left[y_{i} - \frac{\exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)}{1 + \exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)}\right]\boldsymbol{x}_{i}^{T} \\
&= -\sum_{i=1}^{n}\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{i}\frac{\exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)}{\left(1 + \exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)\right)^{2}}
\end{aligned}$$

Similarly, we can rewite the Hessian matrix in the matrix form as $H\left(\boldsymbol{\beta}\right) = \boldsymbol{X}^{T}\boldsymbol{W}\boldsymbol{X}$, where $\boldsymbol{W}$ is a matrix consisted of $-\frac{\exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)}{\left(1 + \exp\left(\boldsymbol{x}_{i}\boldsymbol{\beta}\right)\right)^{2}}$ as a diagonal while the off-diagonal are 0.

By applying the Newton-Ralphson, we will update the parameters for the iteration t by using $\boldsymbol{\beta}^{(t)} = \boldsymbol{\beta}^{(t - 1)} - \left[H\left(\boldsymbol{\beta}^{(t - 1)}\right)\right]^{-1}\left[\triangledown_{\boldsymbol{\beta}}l\left(\boldsymbol{\beta}^{(t - 1)}\right)\right] = \boldsymbol{\beta}^{(t - 1)} - \left[\boldsymbol{X}^{T}\boldsymbol{W}\boldsymbol{X}\right]^{-1}\boldsymbol{X}^{T}\left(\boldsymbol{Y} - \hat{\boldsymbol{Y}}\right)$.

```{r}

## Q3 --------------------------------------------------------------------------
### Data
#### yi, intercept, x1 (coffee assumption), x2 (gender)
designMat <- rbind(c(1, 1, 0, 1), c(0, 1, 0, 1), c(1, 1, 2, 1), c(0, 1, 2, 1), 
      c(1, 1, 4, 1), c(0, 1, 4, 1), c(1, 1, 5, 1), c(0, 1, 5, 1),
      c(1, 1, 0, 0), c(0, 1, 0, 0), c(1, 1, 2, 0), c(0, 1, 2, 0), 
      c(1, 1, 4, 0), c(0, 1, 4, 0), c(1, 1, 5, 0), c(0, 1, 5, 0)) %>%
  as.matrix()

repTime <- c(9, 41 - 9, 94, 213 - 94, 53, 127 - 53, 60, 142 - 60,
             11, 67 - 11, 59, 211- 59, 53, 133 - 53, 28, 76 - 28)

designMat <- designMat[rep(1:nrow(designMat), times = repTime), ]
```

```{r}

### Run the optimization
resultQ3 <- optimQ3(desMat = designMat[, -1], Y = designMat[, 1],
                    b0 = c(0, 0, 0), eps = 1e-10)
```

The stopping criteria for the optimization is that we will say that the result is converged if the Euclidean distance between $\boldsymbol{b}_{t}$ and $\boldsymbol{b}_{t-1}$ less than $\epsilon$. I have set the $\epsilon$ to be $1 \times 10^{-10}$ and let $\boldsymbol{b}_{0}$ to be $\left[0, 0, 0\right]^{T}$.

The result from the optimization shows that the logistic regression is $\log\left(\frac{\hat{p}}{1-\hat{p}}\right) = -1.1878 + 0.1382 x_{i, \text{coffee}} + 0.3973 x_{i, \text{gender}}$. Besides, the algorithm use 5 iterations until the result converges. Note than $x_{i, \text{gender}} = 1$ refers to male, and 0 for female.

### (c)

Figure 5 shows the estimated probability of getting cancer, which is calculated from the estimated model in part (b). We can calculated the probability by using $\frac{\exp\left(\boldsymbol{x\hat{\beta}}\right)}{1 + \exp\left(\boldsymbol{x\hat{\beta}}\right)}$.

```{r, fig.cap = "The estimated probability of getting the cancer for each gender"}

### Plot
designMatPred <- rbind(c(1, 0, 1), c(1, 2, 1), c(1, 4, 1), c(1, 5, 1), 
                       c(1, 0, 0), c(1, 2, 0), c(1, 4, 0), c(1, 5, 0)) %>%
  as.matrix()

data.frame(designMatPred[, -1], 
           p = exp(designMatPred %*% resultQ3$bt)/(1 + exp(designMatPred %*% resultQ3$bt))) %>%
  ggplot(aes(x = X1, y = p, color = factor(X2, labels = c("Female", "Male")))) +
  geom_point() +
  geom_line() +
  ylim(0, 0.5) +
  theme_bw() +
  labs(x = "Coffee Consumed", y = "Probability of getting cancer",
       color = "Gender", title = "Estimated Probability of getting cancer for each gender") + 
  scale_color_manual(values=c("hotpink1", "royalblue2"))

```

According to the model, we can interpret in terms of log odds ratio or the odds. Below are the interpretation in both ways:

  - For the same coffee level consumption, we expected to see the log odd ratio for male is higher than females around 0.3973
  - The odds of male getting the cancer is higher than the females by 1.4878 times given that these two people have the same level of the coffee assumption.

Therefore, we can say that if we compare the chances of a man developing cancer to a woman, and both are drinking the same amount of coffee, the odds of the man getting cancer are about 1.4878 times higher than the odds for the woman. The plot also shows the similar result in term of the probability. It suggests that, with the same coffee consumption, men may have a somewhat higher probability of developing cancer compared to women.

Also, if we are interested in the coffee consumption, these are the statistical interpretations:

  - For individuals with the same gender (male/female), an increase of one unit in coffee consumption is associated with a 0.1382 increase in the logarithm of the odds ratio for cancer.
  - For individuals with the same gender, the odds of developing cancer are 1.1482 times higher for those who consume one additional unit of coffee compared to another individual.
  
To conclude, if two people have the same gender, and one of them drinks one more cup of coffee than the other, the person who drinks more coffee has about 15% higher odds of developing cancer compared to the person who drinks less coffee.

### (d)

In this question, we would like to test that $H_{0}: \beta_{j} = 0$ for $j= 1, 2, 3$. We can use z-statistics to test these null hypothesis.

First, we need to calculate $z_{j} = \frac{\hat{b}_{j}}{\hat{\sigma}_{j}}$. We will use $b^{(t)}_{j}$ as $\hat{b}_{j}$. For the $\hat{\sigma}_{j}$, we can get it by first calculating $\left(\boldsymbol{X}^{T}\boldsymbol{W}\boldsymbol{X}\right)^{-1}$ where $\boldsymbol{W}$ is the result from the optimization. Then, $\hat{\sigma}$ are a square root of the diagonal elements.

```{r}

### Calculate the test statistics for testing H0: beta = 0
est_SD <- sqrt(diag(solve(t(designMat[, -1]) %*% resultQ3$W %*% designMat[, -1])))
z_stat <- (resultQ3$bt - 0)/est_SD

### Compare with z-statistics
# (qnorm(0.05/2) <= z_stat) & (z_stat <= qnorm(1 - (0.05/2))) ## If TRUE, FTR H0
```

Therefore, we have $z_{1} = -7.5508, z_{2} = 3.2368, z_{3} = 2.9714$. We will reject $H_{0}: \beta_{j} = 0$ if $|z_{j}| > z_{1- \frac{0.05}{2}}$ where $z_{1- \frac{0.05}{2}} = 1.96$.

According to the result, we notice that we reject $H_{0}$ for all coefficients. Hence, we can conclude that all regression coefficients are significantly different from 0.

\newpage

## Appendix

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE} 

```
